{"docstore/metadata": {"a5a87296-1f0f-493a-9993-7507bf7fffc8": {"doc_hash": "387f0ab13e458959982a93912ef9ad7129fbc8f20578736569e9ec091e47ecde"}, "2acfa241-c31d-4992-a5a3-3eee9906c429": {"doc_hash": "d442573679bec0965d4bd353229d1b4d0084c428acaa7362087046f6b755cb76"}, "3fed9e7e-3763-4ec2-8cbd-f5f79a241ba1": {"doc_hash": "2a2acb90686b5a436d54669ebd5602188d3e2b4a753ce22a9c4febc49669fe1a"}, "970e1d9a-aea0-4fca-aa3f-8e1f5772664f": {"doc_hash": "8ee0fe2b3bc2afd32cc260d98a1787ad56a32e90a95739a6e695658ab786c4a9"}, "da4ea11c-532c-4014-a5cb-cbfa9cd90f03": {"doc_hash": "38cea742497dc7a88d7b79047decdfa3678e99a91577e09756b2820ceb9c5166"}, "5ba7a2e8-9091-4e6c-8167-2f6429ffb3c4": {"doc_hash": "f7e5d43566610013177b9751914494d03c9fbc2c0dd22a79c26e14cbbe14eded"}, "8d0ea1c8-0074-4ac1-94b5-700a3c153d83": {"doc_hash": "409f6f94527026248018b88cd339f50fef0b55e7757cdb749a269f6f356afe5f"}, "195269a1-8f28-452e-b64a-7ba3a599d200": {"doc_hash": "b2b279f9f119b9260ff837057f01b278b7a258c0b952e543c48d7d730b6ba430"}, "644bfaeb-5cd3-43d9-84ec-67f243101da0": {"doc_hash": "3e97532b6e873018328fdcf54e69de34c90b4afe3eb27fc37dc5b48c31706c84"}, "9953c556-80a8-4412-8976-69f80e5b5eb1": {"doc_hash": "65faf269cf681eb1ffbf12f0dd6183711fea55d86e2d762172ad0ed2721b2843"}, "38f42199-9642-4448-b4ff-21d701875616": {"doc_hash": "3a65817eceebb1e730f68da8e4dfa5ac8faf056c6ad6e955a26cb529ef18baf5", "ref_doc_id": "a5a87296-1f0f-493a-9993-7507bf7fffc8"}, "0fda1a13-6032-4d24-9ab8-89e0a340b9d0": {"doc_hash": "b0cdf7decc4ac2241e68c379155e90326aeaf0336c3d16fb4e7b7ca75ab74756", "ref_doc_id": "2acfa241-c31d-4992-a5a3-3eee9906c429"}, "d0213139-26b5-4221-ba0b-48c9cc48eb2b": {"doc_hash": "907c3bdee15d7bb05bdc8d57c2e078a5adea084799e3fcb47a2ebbb4d8a5d901", "ref_doc_id": "3fed9e7e-3763-4ec2-8cbd-f5f79a241ba1"}, "43e692ee-7d16-4f68-99aa-1b7d9101344e": {"doc_hash": "0956abc56fa06af924afb9799684719d8db9c2c48879f256eedfb522d3290fa8", "ref_doc_id": "970e1d9a-aea0-4fca-aa3f-8e1f5772664f"}, "5c005ddb-bba5-48e1-8a8a-c3487c5b1ae9": {"doc_hash": "42361e471781b91a07dda1a62f4fb9f3c9a6740b69b03abb31afd1c88ebc49f7", "ref_doc_id": "da4ea11c-532c-4014-a5cb-cbfa9cd90f03"}, "7058f3c0-4a64-4bd2-84f8-216375c1bc8e": {"doc_hash": "ab1804f222a3cd2c552915074862291dc0a2fd38cce33df1a466df431923c796", "ref_doc_id": "5ba7a2e8-9091-4e6c-8167-2f6429ffb3c4"}, "fe77f039-dd10-4aa7-a0df-535985ec9cb8": {"doc_hash": "cea42727af90786d8801bdcb869b218a0aedbe2e6d3c91a915dec3cea34c2770", "ref_doc_id": "5ba7a2e8-9091-4e6c-8167-2f6429ffb3c4"}, "8b1d3696-d46e-4ff4-a422-776cd58cbd86": {"doc_hash": "f7a96dc9d8acea6f28e8a91ab2c1d98f6d32b7d0a3fcd13611d9741f361d8890", "ref_doc_id": "8d0ea1c8-0074-4ac1-94b5-700a3c153d83"}, "c8f88e27-430b-43a6-a480-af7cf560e283": {"doc_hash": "489f64fd00e61dc77f3e554cc77cd3790b68d7d74e7737979218021253a132ae", "ref_doc_id": "8d0ea1c8-0074-4ac1-94b5-700a3c153d83"}, "0f3d403e-1625-439d-85e4-1d770df22468": {"doc_hash": "f91c09ce9259ac71209d0c374552564bd5b9aa29ffe8e47c7558227190e15b1c", "ref_doc_id": "195269a1-8f28-452e-b64a-7ba3a599d200"}, "e80b0c99-0e5c-4038-baca-e09dbc6963c2": {"doc_hash": "f6179825c724e3692b5b0b9db07396473078cf1942344d202c48e9f3f3d1a953", "ref_doc_id": "644bfaeb-5cd3-43d9-84ec-67f243101da0"}, "7d8c1a28-2e61-4449-925d-25f15c28925b": {"doc_hash": "38731009cd39a2e1b1c433e22d8f74a23121c52ec3378797cc756f358e7f2cb8", "ref_doc_id": "9953c556-80a8-4412-8976-69f80e5b5eb1"}, "b09f04a3-8968-457e-884a-862d68173a02": {"doc_hash": "c5892579aba72df9aa035510bd12c57b0065af57884ef3790238ce68114ac2a0", "ref_doc_id": "9953c556-80a8-4412-8976-69f80e5b5eb1"}, "da64c7b1-2774-43a8-8a8c-38f94c9fcb28": {"doc_hash": "91879c024c6b27f6f80ab014e4b9925adc3838b5ce2d0f9f4ec784fe2f5620d7", "ref_doc_id": "9953c556-80a8-4412-8976-69f80e5b5eb1"}}, "docstore/ref_doc_info": {"a5a87296-1f0f-493a-9993-7507bf7fffc8": {"node_ids": ["38f42199-9642-4448-b4ff-21d701875616"], "metadata": {"page_label": "1", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}}, "2acfa241-c31d-4992-a5a3-3eee9906c429": {"node_ids": ["0fda1a13-6032-4d24-9ab8-89e0a340b9d0"], "metadata": {"page_label": "2", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}}, "3fed9e7e-3763-4ec2-8cbd-f5f79a241ba1": {"node_ids": ["d0213139-26b5-4221-ba0b-48c9cc48eb2b"], "metadata": {"page_label": "3", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}}, "970e1d9a-aea0-4fca-aa3f-8e1f5772664f": {"node_ids": ["43e692ee-7d16-4f68-99aa-1b7d9101344e"], "metadata": {"page_label": "4", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}}, "da4ea11c-532c-4014-a5cb-cbfa9cd90f03": {"node_ids": ["5c005ddb-bba5-48e1-8a8a-c3487c5b1ae9"], "metadata": {"page_label": "5", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}}, "5ba7a2e8-9091-4e6c-8167-2f6429ffb3c4": {"node_ids": ["7058f3c0-4a64-4bd2-84f8-216375c1bc8e", "fe77f039-dd10-4aa7-a0df-535985ec9cb8"], "metadata": {"page_label": "6", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}}, "8d0ea1c8-0074-4ac1-94b5-700a3c153d83": {"node_ids": ["8b1d3696-d46e-4ff4-a422-776cd58cbd86", "c8f88e27-430b-43a6-a480-af7cf560e283"], "metadata": {"page_label": "7", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}}, "195269a1-8f28-452e-b64a-7ba3a599d200": {"node_ids": ["0f3d403e-1625-439d-85e4-1d770df22468"], "metadata": {"page_label": "8", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}}, "644bfaeb-5cd3-43d9-84ec-67f243101da0": {"node_ids": ["e80b0c99-0e5c-4038-baca-e09dbc6963c2"], "metadata": {"page_label": "9", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}}, "9953c556-80a8-4412-8976-69f80e5b5eb1": {"node_ids": ["7d8c1a28-2e61-4449-925d-25f15c28925b", "b09f04a3-8968-457e-884a-862d68173a02", "da64c7b1-2774-43a8-8a8c-38f94c9fcb28"], "metadata": {"page_label": "10", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}}}, "docstore/data": {"38f42199-9642-4448-b4ff-21d701875616": {"__data__": {"id_": "38f42199-9642-4448-b4ff-21d701875616", "embedding": null, "metadata": {"page_label": "1", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a5a87296-1f0f-493a-9993-7507bf7fffc8", "node_type": "4", "metadata": {"page_label": "1", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "hash": "387f0ab13e458959982a93912ef9ad7129fbc8f20578736569e9ec091e47ecde", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A Survey on Retrieval\n-\nAugmented \nGeneration: From Naive to Adaptive \nApproaches with Financial Insights\n \nYug V. Patel\n1\n, \nDr\n.\n \nVijaykumar Salvia\n2\n,\n \nand \nDr\n.\n \nIndrajeet Kumar\n3    \n \n                                                                                                                             \n                         \n \n1 \nM. Tech\n \nStudent, Department of CSE, Parul University, Vadodara, Gujarat, India \n \n2 \nProfessor, Department of CSE, Parul University, Vadodara, Gujarat, India \n \nAbstract\n: \nThis survey examines the evolving landscape of Retrieval\n-\nAugmented Generation (RAG) systems, from naive \napproaches to adaptive and specialized implementations, \nfocusing\n \non financial applications. We explore various RAG \narchitectures including Naive RAG [1]\n, Advanced RAG [1], Modular RAG [2], Adaptive RAG [3], Corrective RAG [4], Self\n-\nRAG [5], Hybrid RAG [6], and Graph RAG [24,25]. The paper delves into retrieval methods, including dense [8,9] and sparse \n[10,11] techniques, and discusses augmentation strateg\nies such as zero\n-\nshot [16] and few\n-\nshot [17] prompting. We \nanalyze\n \nthe \nFinanceBench dataset [20] as a case study, highlighting challenges in \nanswering financial questions\n \nand proposing future \ndirections for RAG systems in finance. The survey emphasizes the\n \npotential of domain\n-\nspecific fine\n-\ntuning [23] and hybrid \nretrieval methods [6] to enhance RAG performance in complex financial contexts.\n \n \nKeywords\n:\n \nAI, \nFinanceBench, \nRetrieval Augmented \nG\neneration (RAG)\n.\n \n1.\n \nINTRODUCTION\n \nIn natural language processing, \nparticularly in question answering, RAG (Retrieval Augmented Generation) \nstands out as a distinctive approach that merges the strengths of both retrieval\n-\nbased and generation\n-\nbased \ntechniques. Traditional Q&A systems primarily rely on these two strategies:\n \nretrieval\n-\nbased and generation\n-\nbased \nmethods.\n \nFinding pertinent sections or documents within a huge corpus of text is the foundation of retrieval\n-\nbased \napproaches, which select the best response from these retrieved sources. Although these techniques are \neffective \nin locating pertinent data, their applicability may be hampered by the corpus's coverage and the quality of the \nretrieval procedure. On the other hand, generation\n-\nbased approaches create responses from the ground up \ndepending on the context and t\nhe input question. Although these techniques can produce \nvarious\n \ncontextually \nrelevant solutions\n, they may not always be successful.\n \nKRONIKA JOURNAL(ISSN NO-0023:4923)  VOLUME 25 ISSUE 1 2025\nPAGE NO: 53", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2598, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0fda1a13-6032-4d24-9ab8-89e0a340b9d0": {"__data__": {"id_": "0fda1a13-6032-4d24-9ab8-89e0a340b9d0", "embedding": null, "metadata": {"page_label": "2", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2acfa241-c31d-4992-a5a3-3eee9906c429", "node_type": "4", "metadata": {"page_label": "2", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "hash": "d442573679bec0965d4bd353229d1b4d0084c428acaa7362087046f6b755cb76", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.\n \nAN OVERVIEW OF RAG ARCHITECTURE\n \n2.1 \nNa\u00efve Retrieval Augmented Generation (RAG)\n \n \nFig. 1\n. Architecture of Naive \nRetrieval\n-\nAugmented Generation (RAG) System\n.\n \nIn the paper \n[1]\n \nNaive RAG is described as a basic form of the Retrieval\n-\nAugmented Generation framework. It \nprimarily involves a straightforward integration of a \nretrieval system with a generative model, where the \nretriever fetches relevant documents or passages based on the input query. These documents are then used by \nthe generative model to enhance the output with accurate and relevant information. Naive RAG is \nconsidered \nfoundational and serves as a crucial baseline for evaluating more complex RAG systems. Despite its simplicity, \nit effectively demonstrates the benefits of incorporating external knowledge into generative models, particularly \nfor knowledge\n-\nintens\nive tasks.\n \n2.2 \nAdvanced RAG\n \nAdvanced Retrieval\n-\nAugmented Generation (Advanced RAG) represents a significant evolution in integrating \nexternal knowledge into Large Language Models (LLMs). It enhances the basic RAG framework with more \nsophisticated retrieval\n \nand generation techniques, addressing issues of information accuracy and content \nrelevance \n[1]\n. \n \nFig. 2\n. Enhanced Architecture of Advanced Retrieval\n-\nAugme\nnted Generation (RAG) System\n.\n \nKRONIKA JOURNAL(ISSN NO-0023:4923)  VOLUME 25 ISSUE 1 2025\nPAGE NO: 54", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1397, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d0213139-26b5-4221-ba0b-48c9cc48eb2b": {"__data__": {"id_": "d0213139-26b5-4221-ba0b-48c9cc48eb2b", "embedding": null, "metadata": {"page_label": "3", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3fed9e7e-3763-4ec2-8cbd-f5f79a241ba1", "node_type": "4", "metadata": {"page_label": "3", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "hash": "2a2acb90686b5a436d54669ebd5602188d3e2b4a753ce22a9c4febc49669fe1a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Advanced RAG is particularly effective for tasks requiring high precision and domain\n-\nspecific knowledge. It \nemploys improved algorithms for retrieving pertinent information from vast databases and generating contextually \nappro\npriate, factually accurate responses. This advancement leads to more dynamic and reliable outputs, reducing \ncommon LLM problems like hallucination and outdated information. By refining both retrieval and generation \nprocesses, Advanced RAG bridges the gap b\netween extensive knowledge repositories and LLM capabilities. This \nprogress has significant implications for various applications, from enhancing question\n-\nanswering systems to \nimproving automated content generation in specialized fields. \nAdvanced RAG repre\nsents a significant \nbreakthrough in natural language processing, leading to the creation of more intelligent and context\n-\nsensitive AI \nsystems.\n \n2.3 \nModular RAG\n \nModular RAG systems are engineered to boost the effectiveness of large language models by \nintegrating retrieval \nmechanisms that actively fetch pertinent information from databases or extensive text collections. This \ncombination greatly improves the precision and relevance of the content produced by these models.\n \nThe term \n\"modular\" in RAG system\ns highlights their flexible architecture, enabling various system components to be easily \nreconfigured or swapped out to meet specific tasks or requirements. This modularity ensures that RAG systems \nare versatile and adaptable, making them suitable for a b\nroad spectrum of applications, ranging from natural \nlanguage processing tasks to more intricate data handling and content generation across different domains.\n[1], [2]\n \n2.4 \nAdaptive RAG\n \nAdaptive\n-\nRAG is an innovative QA framework that adjusts dynamically to select the most suitable method based \non the complexity of the query, t\nhereby enhancing the performance of retrieval\n-\naugmented large language models \n(LLMs). The system utilizes a classifier, a smaller language model, to assess the complexity of incoming queries.\n \nThis allows the system to choose between several tactics, from s\ntraightforward no\n-\nretrieval procedures to more \nintricate iterative retrieval\n-\naugmented options. The framework fluidly adjusts to different query complexities, \nstriving to strike a compromise between accuracy and processing performance. It has been proven t\no be more \naccurate and efficient than current techniques on \nseveral\n \nopen\n-\ndomain quality assurance datasets.\n[3]\n \n \nFig. 3\n. Workflow of Adaptive Retrie\nval\n-\nAugmented Generation (RAG) Framework\n.\n \n2.5 \nCorrective RAG\n \nCorrective Retrieval\n-\nAugmented Generation, or CRAG, has been a notable development in RAG frameworks \nrecently, offering a substantial enhancement over the intrinsic drawbacks of RAG models. \nTo\n \ninitiate targeted \nremedial activities, CRAG presents a lightweight retrieval evaluator that rates the relevance of retrieved \nKRONIKA JOURNAL(ISSN NO-0023:4923)  VOLUME 25 ISSUE 1 2025\nPAGE NO: 55", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3018, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "43e692ee-7d16-4f68-99aa-1b7d9101344e": {"__data__": {"id_": "43e692ee-7d16-4f68-99aa-1b7d9101344e", "embedding": null, "metadata": {"page_label": "4", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "970e1d9a-aea0-4fca-aa3f-8e1f5772664f", "node_type": "4", "metadata": {"page_label": "4", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "hash": "8ee0fe2b3bc2afd32cc260d98a1787ad56a32e90a95739a6e695658ab786c4a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "documents and provides confidence scores. This reduces the \n \nFig. 4\n. Process Flow of Corrective Retrieval\n-\nAugmented Generation (CR\nAG) System\n.\n \n \npossibility of including erroneous data by guaranteeing that only the most pertinent information is used during the \ngenerating process.\n[4]\n \nFurthermore, CRAG supplements static corp\nora with large\n-\nscale web searches to increase \nthe accuracy and diversity of the knowledge base. The decompose\n-\nthen\n-\nrecompose algorithm, which eliminates \nunnecessary information to improve the accuracy and efficiency of knowledge consumption, is a fundament\nal \ninnovation of CRAG.\n \n2.6 \nSelf\n-\nRAG\n \nSelf\n-\nRAG signifies a major advancement in the domain of large language models (LLMs).\n \nBy incorporating \nadaptive retrieval\n \n[3]\n \nand self\n-\nreflection mechanisms, Self\n-\nRAG addresses the limitations of traditional retrieval\n-\naugmented models. The model's ability to dynamically retrieve relevant information and critically evaluate its \nown responses enhances its controllability and adaptability to diverse tasks. Experimental\n \nresults consistently \ndemonstrate Self\n-\nRAG's superiority over standard LLMs and other retrieval\n-\naugmented models, particularly in \nterms of factuality and citation accuracy. This innovative approach is poised to have a profound impact on various \napplication\ns, including open\n-\ndomain question answering, reasoning, fact verification, and long\n-\nform generation\n \n[5]\n.\n \n2.7 \nHybrid RAG\n \nThe Hybrid \nRetrieval\n-\nAugmented Generation (RAG) System represents a notable advancement in optimizing \nlarge language models (LLMs) for retrieval\n-\naugmented generation tasks. This system integrates several crucial \nenhancements aimed at boosting accuracy and addressing \nthe common problem of hallucinations in LLMs\n.\n \nNotable innovations include optimized retrieval processes that efficiently leverage text chunks and tables from \nweb sources, the integration of attribute predictors to reduce errors, and the deployment of both \na Large Language \nModel Knowledge Extractor and a Knowledge Graph Extractor. These components collaboratively enhance the \nmodel's reasoning abilities and numerical computation accuracy. The effectiveness of this hybrid system was \ndemonstrated during the Met\na CRAG KDD Cup 2024, where it secured a third\n-\nplace finish in Task 1 and achieved \nfirst place in five out of seven question types in Task 2, competing against a robust field of over 2,000 participants \nand 5,500 submissions. This hybrid approach not only el\nevates model performance but also establishes a new \nstandard for complex reasoning tasks in computational models\n[6]\n.\n \n2.8 \nAgentic RAG\n \nThe survey of LLM\n-\nbased Multi\n-\nAgents\n \nprovides a valuable foundation for understanding the broader context of \nLLMs in multi\n-\nagent systems\n[7]\n. While the survey does not explicitly address Agentic RAG, it offers insights \ninto the planning, reasoning, and communication capabilities of LLMs, which are essential for integrating RAG \nsystems into complex problem\n-\nsolving environments. The s\nurvey's discussion of LLM profiling and \nKRONIKA JOURNAL(ISSN NO-0023:4923)  VOLUME 25 ISSUE 1 2025\nPAGE NO: 56", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3188, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5c005ddb-bba5-48e1-8a8a-c3487c5b1ae9": {"__data__": {"id_": "5c005ddb-bba5-48e1-8a8a-c3487c5b1ae9", "embedding": null, "metadata": {"page_label": "5", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da4ea11c-532c-4014-a5cb-cbfa9cd90f03", "node_type": "4", "metadata": {"page_label": "5", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "hash": "38cea742497dc7a88d7b79047decdfa3678e99a91577e09756b2820ceb9c5166", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "communication within multi\n-\nagent frameworks provides a useful starting point for exploring the potential \napplications of Agentic RAG in enhancing retrieval and generation processes.\n \n2.9 \nGraph RAG\n \nGraph Retrieval\n-\nAugm\nented Generation (Graph RAG) emerges as a significant advancement in RAG systems, \nspecifically designed to tackle limitations of LLMs\n \nlike hallucinations and outdated information. this approach \nutilizes structured knowledge graphs for precise information r\netrieval, mitigating issues found in traditional RAG. \nIt works through three key steps: 1) Graph\n-\nBased Indexing structures knowledge for efficient retrieval, 2) Graph\n-\nGuided Retrieval leverages connections within the graph to find relevant information, and\n \n3) Graph\n-\nEnhanced \nGeneration integrates this information for accurate and contextually rich LLM outputs. These surveys highlight \nthe potential of Graph RAG across various domains while outlining challenges and future research directions. \nIntegrating Graph\n \nRAG with LLMs promises significant improvements in applicability and accuracy for complex \ntasks and queries.\n \n3.\n \nRetrieval\n \nIn RAG, retrieval involves sourcing relevant information from external databases to enhance the performance of \nlanguage models. This \nmechanism is essential for increasing the accuracy and relevance of generated responses, \nespecially in complex tasks such as relation extraction and question answering\n.\n \n3.1 \nDense Retrieval\n \nDense retrieval involves using dense vector representations of quer\nies and documents, often obtained through \nneural networks, to find the most relevant documents.\n \n3.1.1\n \nDual\n-\nEncoder Models.\n \nThe paper [8] presents an innovative method for passage retrieval utilizing dense representations, which are \ndeveloped through a dual\n-\nencod\ner model. This approach significantly boosts the efficiency of open\n-\ndomain \nquestion answering systems by shifting from traditional sparse vector space models such as TF\n-\nIDF or BM25. \nThe dual\n-\nencoder framework functions by separately encoding questions and \npassages into dense vectors, which \nare then employed to calculate similarity scores for retrieval tasks. This technique enables a more sophisticated \nunderstanding and matching based on semantic similarities rather than just keyword overlap. The effectivene\nss of \nthis approach is evidenced by its superior performance compared to a robust Lucene\n-\nBM25 system, achieving \nabsolute improvements of 9%\n-\n19% in top 20 passage retrieval accuracy across various open\n-\ndomain QA datasets.\n \n \n3.1.2\n \nContextualized Embeddings\n.\n \nIn \ntheir 2019 paper, \"Latent Retrieval for Weakly Supervised Open Domain Question Answering\" [9], Lee et al. \npropose a groundbreaking method for open\n-\ndomain question answering (QA) that moves away from the \nconventional reliance on highly supervised evidence a\nnd opaque information retrieval (IR) systems.\n \nThey present \na method where both the retriever and reader components are trained together directly from question\n-\nanswer pairs, \neliminating the need for an existing IR system. This technique shifts the focus by \ntreating evidence retrieval from \nvast sources like Wikipedia as a latent variable, moving away from the conventional need for explicit evidence \nsupervision. To support this learning approach, the authors pre\n-\ntrain the retriever using an Inverse Cloze Task,\n \nwhich effectively captures contextualized embeddings that are pertinent to the questions. This method has been \nproven to significantly outperform traditional IR systems, such as BM25, particularly in scenarios where users are \nactively searching for inform\nation, with improvements of up to 19 points in exact match scores. This advancement \nhighlights the crucial role of contextualized embeddings in enhancing retrieval accuracy in weakly supervised \nsettings, making it an essential element for systems aiming to\n \nadvance open\n-\ndomain QA\n.\n \n \n3.2\n \nSparse Retrieval\n \n3.2.1 \nBM25 Algorithm.\n \n \nBM25 is a well\n-\nestablished algorithm in the field of information retrieval, particularly noted for its effectiveness \nin ranking documents based on their relevance to a given search query. Originating from the probabilistic retrieval \nframework, BM25 calcula\ntes the relevance score of documents by considering term frequency (the number of \ntimes a term appears in a document) and inverse document frequency (how common or rare a term is across all \ndocuments in the collection). This balance helps to address the is\nsues of term frequency saturation and the varying \nKRONIKA JOURNAL(ISSN NO-0023:4923)  VOLUME 25 ISSUE 1 2025\nPAGE NO: 57", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4647, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7058f3c0-4a64-4bd2-84f8-216375c1bc8e": {"__data__": {"id_": "7058f3c0-4a64-4bd2-84f8-216375c1bc8e", "embedding": null, "metadata": {"page_label": "6", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ba7a2e8-9091-4e6c-8167-2f6429ffb3c4", "node_type": "4", "metadata": {"page_label": "6", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "hash": "f7e5d43566610013177b9751914494d03c9fbc2c0dd22a79c26e14cbbe14eded", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe77f039-dd10-4aa7-a0df-535985ec9cb8", "node_type": "1", "metadata": {}, "hash": "c6c0114f35094673e5f904a28cf2c7f2a34676646c135dd17785a4f1e00aa746", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "document lengths. The effectiveness of BM25 in legal case retrieval is underscored by its performance in the \nCOLIEE 2021 competition, where a straightforward implementation of BM25 achieved second place, \nde\nmonstrating its robustness and reliability as a baseline method in complex domains such as legal document \nretrieval \n[8]\n.\n \n \n3.2.2\n \nTF\n-\nIDF Vecto\nrization.\n \nTF\n-\nIDF (Term Frequency\n-\nInverse Document Frequency) vectorization is a fundamental technique in automatic \ntext retrieval, widely recogniz\ned for its importance in the literature. It is a statistical method used to assess the \nsignificance of a word in a document relative to a collection of documents (or corpus). The primary concept is to \nbalance the frequency of a term in a specific document \n(term frequency, TF) with how common or rare the term \nis across all documents (inverse document frequency, IDF). This balance helps in pinpointing terms that are both \nrelevant and distinctive for a given document.\n \nThe TF\n-\nIDF methodology comprises two main \ncomponents:\n \nTerm Frequency (TF): This measures the frequency of a term in a document. The underlying assumption is that \nthe more frequently a term appears in a document, the more important it is. However, this frequency is normalized \nto avoid bias towards \nlonger documents.\n \nInverse Document Frequency (IDF): This component assesses the importance of a term within the entire corpus. \nIt operates on the assumption that terms appearing in many documents are less informative than those appearing \nin fewer documents\n. IDF is mathematically calculated as the logarithm of the total number of documents divided \nby the number of documents containing the term.\n \nThe TF\n-\nIDF score is obtained by multiplying the TF and IDF values for each term, resulting in a weight that \nreflect\ns the term's significance in the document while diminishing the weight of commonly used terms that are \nless informative.\n \nThis approach is particularly notable for its simplicity and effectiveness. Despite being developed decades ago, \nTF\n-\nIDF remains a corne\nrstone technique in text mining and information retrieval. One might question its \nperformance in modern real\n-\nworld scenarios, especially with the emergence of advanced models like word \nembeddings and transformers. Nonetheless, TF\n-\nIDF's interpretability and\n \nease of implementation make it a \npreferred method for many applications, including search engines and document clustering.\n \nCompared to other term\n-\nweighting methods, TF\n-\nIDF offers a balance between computational efficiency and \nretrieval performance. While \nmore sophisticated models might provide enhancements in certain contexts, TF\n-\nIDF's robustness and simplicity ensure its continued relevance. The insights gained from TF\n-\nIDF vectorization \nhave paved the way for more advanced techniques, offering a baseline \nagainst which newer methods can be \nevaluated\n \n[9]\n.\n \n \n3.\n3 \n \nHybrid \nRetrieval\n \nHybrid retrieval, as discussed in the context \nof \nRAG\n, represents a sophisticated blend of techniques aimed at \nenhancing the performance of \nLLMs \nand AI\n-\ngenerated content systems. \nThis method combines conventional \nretrieval techniques with advanced, adaptive retrieval strategies to create a robust framework f\nor extracting and \nintegrating information. The core strength of hybrid retrieval is its capability to not only gather relevant data but \nalso to dynamically refine the retrieval process according to the context or specific requirements of the task [1].\n \nWhat\n \nwe find particularly intriguing about this approach is its potential to significantly reduce the common pitfalls \nassociated with large language models, such as the generation of outdated or irrelevant content. By integrating a \nhybrid retrieval mechanism, \nthese models can dynamically access the most current and relevant information, \npotentially leading to more accurate and contextually appropriate outputs. One might wonder how this method \nwould perform in real\n-\nworld scenarios where the demand for up\n-\nto\n-\nthe\n-\nminute information is critical, such as in \nnews generation or financial forecasting.\n \nMoreover, hybrid retrieval could be seen as a bridge between static knowledge bases and the fluid, ever\n-\nchanging \nnature of real\n-\nworld data. It seems that this approach not\n \nonly supports the generation of more credible and reliable \ncontent but also enhances the learning capabilities of the model by exposing it to a broader array of data sources.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4509, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fe77f039-dd10-4aa7-a0df-535985ec9cb8": {"__data__": {"id_": "fe77f039-dd10-4aa7-a0df-535985ec9cb8", "embedding": null, "metadata": {"page_label": "6", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ba7a2e8-9091-4e6c-8167-2f6429ffb3c4", "node_type": "4", "metadata": {"page_label": "6", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "hash": "f7e5d43566610013177b9751914494d03c9fbc2c0dd22a79c26e14cbbe14eded", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7058f3c0-4a64-4bd2-84f8-216375c1bc8e", "node_type": "1", "metadata": {"page_label": "6", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "hash": "ab1804f222a3cd2c552915074862291dc0a2fd38cce33df1a466df431923c796", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "What\n \nwe find particularly intriguing about this approach is its potential to significantly reduce the common pitfalls \nassociated with large language models, such as the generation of outdated or irrelevant content. By integrating a \nhybrid retrieval mechanism, \nthese models can dynamically access the most current and relevant information, \npotentially leading to more accurate and contextually appropriate outputs. One might wonder how this method \nwould perform in real\n-\nworld scenarios where the demand for up\n-\nto\n-\nthe\n-\nminute information is critical, such as in \nnews generation or financial forecasting.\n \nMoreover, hybrid retrieval could be seen as a bridge between static knowledge bases and the fluid, ever\n-\nchanging \nnature of real\n-\nworld data. It seems that this approach not\n \nonly supports the generation of more credible and reliable \ncontent but also enhances the learning capabilities of the model by exposing it to a broader array of data sources. \nThe results suggest that hybrid retrieval could pave the way for more sophistica\nted, adaptable, and efficient AI \nsystems, which could be a game\n-\nchanger in fields requiring high levels of accuracy and timeliness in information \nretrieval and utilization\n[10]\n.\n \nKRONIKA JOURNAL(ISSN NO-0023:4923)  VOLUME 25 ISSUE 1 2025\nPAGE NO: 58", "mimetype": "text/plain", "start_char_idx": 3536, "end_char_idx": 4847, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8b1d3696-d46e-4ff4-a422-776cd58cbd86": {"__data__": {"id_": "8b1d3696-d46e-4ff4-a422-776cd58cbd86", "embedding": null, "metadata": {"page_label": "7", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8d0ea1c8-0074-4ac1-94b5-700a3c153d83", "node_type": "4", "metadata": {"page_label": "7", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "hash": "409f6f94527026248018b88cd339f50fef0b55e7757cdb749a269f6f356afe5f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8f88e27-430b-43a6-a480-af7cf560e283", "node_type": "1", "metadata": {}, "hash": "8fd5d8f1cfdb7bd3098acdf6c9702e4f7b22cf082c77aef86dfcb5b6565a43f4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.\n4\n \nRetriever Fine\nt\nuning\n \nRetriever fine\n-\ntuning in RAG is pivotal for enhancing the performance of AI\n-\ngenerated content systems by \noptimizing the retrieval component. This process involves refining the retriever's parameters to more effectively \nselect relevant information from a dataset, which then informs the generation process. The significance of this \nmethod lies in its capacity to\n \ndynamically update and adapt to new information, thereby maintaining the relevance \nand accuracy of the generated content over time.\n \nThe methodology typically involves training the retriever on a specific task or dataset to improve its ability to \nidentify \nand retrieve the most pertinent data. This step is crucial because the quality of the retrieved input greatly \nimpacts the output of the generator. By fine\n-\ntuning the retriever, the system can produce more accurate and \ncontextually appropriate responses. Fu\nrthermore, this approach represents an improvement over traditional \nmethods that might rely on static or less adaptive retrieval mechanisms \n[10]\n.\n \n4.\n \nAUGMENTATION\n \nIt \nrefers\n \nto combining the user query with the retrieved context within a single template. This template also \nincludes the instruction, known as a prompt. So ultimately, it's called a prompt template\n[11]\n. The retrieved context \nis simply the information we get from the retrieval process based on the user's qu\nestion\n.\n \nthe technique of defining \na prompt te\nmplate is called prompt engineering\n[12], [13]\n.\n \nthere are \nmany \nways\n \nof defining prompt \ntemplates.\n \n \n \n4.\n1 \nZero\n-\nshort prompting\n \nThis method leverages the extensive pre\n-\ntraining data of LLMs to apply them to new tasks without requiring \nspecific training data for each task. Zero\n-\nshot prompting relies on a single prompt, meticulously crafted to describe \nthe task\n \nat hand, to guide the LLM in generating responses. The paper \n \n[14]\n \nemphasizes the importance of \nd\nesigning effective prompts that clearly communicate the desired task to the model, thereby maximizing \nperformance without the need for additional \nlabelled\n \ntraining examples.\n \nThe primary benefit of zero\n-\nshot prompting is its capacity to quickly adapt large \nlanguage models (LLMs) to new \ntasks, especially in situations where obtaining labeled data is impractical or impossible. However, creating \nprompts that are both clear and accurately representative of the task can be challenging. The effectiveness of the \nmo\ndel's performance is heavily influenced by the quality and specificity of the input prompt. Despite these \nchallenges, zero\n-\nshot prompting has a wide range of potential applications, including answering domain\n-\nspecific \nquestions and generating creative cont\nent.\n \n4.2 \nFew\n-\nshort prompting\n \nThe few\n-\nshot prompt technique\n \nrepresents a compelling approach to leveraging large pre\n-\ntrained language models \n(PLMs) for tasks in low\n-\nresource languages. This technique involves providing the PLM with a minimal number \nof examp\nles (few\n-\nshot) of a specific task in a target language, which serves as a context for the model to learn and \nperform the task on new, unseen data\n \n[15]\n. The beauty of this method lies in its simplicity and efficiency, \nparticularly when resources are scarce or when training data is limited.\n \nWhat we found particularly intriguing about this approach is its potential to\n \ndemocratize access to advanced NLP \ntechnologies across diverse linguistic landscapes. \nUsing only a few examples, the model can effectively adapt to \nnew languages and tasks, marking a significant shift from traditional methods that demand extensive data an\nd \ncomputational resources\n. \n \n5\n. \nG\nENERATION\n \nThe generation\n \npart in RAG \ninvolves\n \ngenerating the answer or response based on \na \ngiven prompt template \nwith \nthe help of\n \nLLM\n \n[16]\n.\n \nThere are two types of LLMs available in the industry\n,\n \none is \nclosed \nsource [\n18]\n,\n \nand the \nother one is \nopen source\n \n[18]\n. \nFor using \nclosed source\n \nLLMs\n,\n \nwe have to pay the amount to the respective \norganization\n \non the other hand \nopensource LLMs \nare\n \ncompletely free\n,\n \nwe can use \nthem\n \nin our local computer \nsystem or by using some inference engine\n \n[17]\n, \nhowever\n \nin the inference engine case\n,\n \nthe \nuser may have to pay \nsome cost associated with inference.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4340, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c8f88e27-430b-43a6-a480-af7cf560e283": {"__data__": {"id_": "c8f88e27-430b-43a6-a480-af7cf560e283", "embedding": null, "metadata": {"page_label": "7", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8d0ea1c8-0074-4ac1-94b5-700a3c153d83", "node_type": "4", "metadata": {"page_label": "7", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "hash": "409f6f94527026248018b88cd339f50fef0b55e7757cdb749a269f6f356afe5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b1d3696-d46e-4ff4-a422-776cd58cbd86", "node_type": "1", "metadata": {"page_label": "7", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "hash": "f7a96dc9d8acea6f28e8a91ab2c1d98f6d32b7d0a3fcd13611d9741f361d8890", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5\n. \nG\nENERATION\n \nThe generation\n \npart in RAG \ninvolves\n \ngenerating the answer or response based on \na \ngiven prompt template \nwith \nthe help of\n \nLLM\n \n[16]\n.\n \nThere are two types of LLMs available in the industry\n,\n \none is \nclosed \nsource [\n18]\n,\n \nand the \nother one is \nopen source\n \n[18]\n. \nFor using \nclosed source\n \nLLMs\n,\n \nwe have to pay the amount to the respective \norganization\n \non the other hand \nopensource LLMs \nare\n \ncompletely free\n,\n \nwe can use \nthem\n \nin our local computer \nsystem or by using some inference engine\n \n[17]\n, \nhowever\n \nin the inference engine case\n,\n \nthe \nuser may have to pay \nsome cost associated with inference.\n \nKRONIKA JOURNAL(ISSN NO-0023:4923)  VOLUME 25 ISSUE 1 2025\nPAGE NO: 59", "mimetype": "text/plain", "start_char_idx": 3682, "end_char_idx": 4413, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0f3d403e-1625-439d-85e4-1d770df22468": {"__data__": {"id_": "0f3d403e-1625-439d-85e4-1d770df22468", "embedding": null, "metadata": {"page_label": "8", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "195269a1-8f28-452e-b64a-7ba3a599d200", "node_type": "4", "metadata": {"page_label": "8", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "hash": "b2b279f9f119b9260ff837057f01b278b7a258c0b952e543c48d7d730b6ba430", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6.\n \nLiterature Review \non\n \nFinanceBench\n \nDataset\n \n6.1 \nOverview \nOf\n \nFinanceBench Dataset\n \nThe FinanceBench dataset serves as a thorough benchmark for assessing the performance of large language \nmodels (LLMs) in financial question answering (QA) \n[18]\n. \nIt comprises 10,231 questions about 40 publicly traded \ncompanies, derived from 361 public filings, including 10\n-\nKs, 10\n-\nQs, and earnings reports, covering the period \nfrom 2015 to 2023.\n \nThe questions are categorized into three types: domain\n-\nrelevant, \nnovel\n-\ngenerated\n, and metrics\n-\ngenerated, ensuring coverage of various financial analysis scenarios. Each question is paired with an answer, \nevidence, and relevant metadata. This benchmark is c\nrucial for assessing LLMs in real\n-\nworld financial contexts, \nespecially in tasks involving numerical reasoning, information extraction, and logical deductions. The dataset \nhighlights the limitations of current LLMs in accurately handling complex financial Q\nA, as even state\n-\nof\n-\nthe\n-\nart \nmodels struggle with tasks requiring in\n-\ndepth financial knowledge and reasoning.\n \n \nKey Findings f\nor the FinanceBench\n \n1.\n \nDocument Chunking for RAG in Financial\n \nReports\n \nAntonio Jimeno Yepes et al. (2024) in \nthe paper \n[19]\n \nintroduced a novel document chunking approach \nspecifically designed for financial documents in RAG systems. Traditional chunking methods often disregard \ndocument structures, leading to suboptimal retrieval. The authors prop\nosed a method that chunks documents \nbased on their structural elements, such as titles and tables, which enhances the relevance and accuracy of the \nretrieved content. This method showed a significant improvement in processing financial documents, which \noft\nen contain complex layouts and dense information.\n \n \n2.\n \nImproving RAG Retrieval for Financial Documents\n \nSpurthi Setty et al. (2024) explored the limitations of traditional RAG pipelines in financial document \nretrieval and proposed enhancements like query expans\nion and re\n-\nranking algorithms. Their study\n \nresearch\n \n[20]\n, \nemphasized that standard retrieval methods often fail due to the complex nature of financial texts. By \nintegrating more sophisticated chunking and re\n-\nranking methods, their approach improved the accuracy and \nrelevance of the information retrieved, there\nby reducing the hallucination problem in LLMs.\n \n \n3.\n \nImpact of Domain\n-\nSpecific Fine\n-\nTuning on RAG Systems\n \nZooey Nguyen et al. (2024) in \nthe paper \n[21]\n \nexamined\n \nthe effects of fine\n-\ntuning both embedding models and \nLLMs on the FinanceBench dataset. The study demonstrated that fine\n-\ntuning significantly improves RAG \nperformance, especially when combined with iterative reasoning frameworks like the OODA loop. Their \nf\nindings suggest that while generic RAG models struggle with domain\n-\nspecific queries, fine\n-\ntuned models \ncan achieve \nnear\n-\nhuman\n-\nexpert\n \naccuracy.\n \n \n6.2 \nChallenges and Limitations\n \nComplex Document Structures\n:\n \nFinancial documents often contain complex structures\n, such as tables, footnotes, \nand multi\n-\npart sections. This complexity poses a significant challenge for AI systems, particularly in accurately \nchunking and retrieving relevant information.\n \nNeed for Domain Expertise\n:\n \nThe dataset's questions require a deep u\nnderstanding of financial concepts, which \ncan be difficult for generic AI models. This limitation has driven research into domain\n-\nspecific fine\n-\ntuning and \nspecialized retrieval strategies.\n \nScalability Issues\n:\n \nAs the dataset is based on publicly available d\nocuments, scaling it to cover more companies or \nadditional years of filings would require significant manual effort, particularly in crafting new questions and \nverifying answers.\n \nHigh computational cost of fine\n-\ntuning\n \nlarge language models (LLMs) on domain\n-\nspecific datasets like \nFinanceBench. Fine\n-\ntuning requires substantial computational resources, including powerful GPUs or TPUs, and \ncan be time\n-\nconsuming, especially when dealing with large models like GPT\n-\n3 or GPT\n-\n4. The cost can be \nKRONIKA JOURNAL(ISSN NO-0023:4923)  VOLUME 25 ISSUE 1 2025\nPAGE NO: 60", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4163, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e80b0c99-0e5c-4038-baca-e09dbc6963c2": {"__data__": {"id_": "e80b0c99-0e5c-4038-baca-e09dbc6963c2", "embedding": null, "metadata": {"page_label": "9", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "644bfaeb-5cd3-43d9-84ec-67f243101da0", "node_type": "4", "metadata": {"page_label": "9", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "hash": "3e97532b6e873018328fdcf54e69de34c90b4afe3eb27fc37dc5b48c31706c84", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "prohibitive for small\ner organizations or research teams, limiting their ability to customize models for specific \nfinancial tasks.\n \n \n6.3 \nFuture Directions\n \nDataset Expansion: Future research could aim to enhance the FinanceBench dataset by incorporating more \ncompanies, extending \nthe coverage to additional years of filings, and including a broader array of financial \ndocuments. Such expansion would further strengthen the dataset's robustness and applicability to a wider range \nof financial analysis tasks\n.\n \nIntegration with Other Datas\nets:\n \nCombining FinanceBench with other financial datasets, such as those focusing \non market data or financial news, could create a more comprehensive benchmark for testing AI systems in finance.\n \nAdvanced RAG Architectures:\n \nResearch into more advanced RAG a\nrchitectures, such as modular \n[2]\n \nor graph\n-\nbased\n \n[22], [23]\n \nmodels could lead to significant improvements in handling the complexities of financial \ndocuments.\n \nImprove the retriever quality by fine\n-\ntuning\n \nor adopting hybrid RAG\n \nretrieval methods.\n \nFine\n-\ntuning the retriever \nmodel\n \n[21]\n \nspecific\nally on financial documents could significantly enhance its ability to identify and retrieve the \nmost relevant information. Additionally, hybrid RAG retrieval techniques\n \n[6]\n, which combine traditional retrieval \nmethods with advanced machine learning models, can be adapted to better handle the complexity of financial data. \nBy i\nntegrating vector\n-\nbased retrieval \n[24]\n \nwith traditional keyword search or incorporating doma\nin\n-\nspecific \nembeddings, hybrid approaches could offer more precise and contextually relevant retrieval, ultimately leading to \nmore accurate and reliable Q&A performance.\n \n \n7.\n \nCONCLUSION\n \nThe FinanceBench dataset marks a major advancement in assessing AI systems\n \nfor financial analysis, providing \na thorough benchmark for evaluating Retrieval\n-\nAugmented Generation (RAG) models and other AI techniques\n. \nThrough a combination of realistic financial scenarios, diverse question types, and a focus on key financial \nstateme\nnts, it provides an in\n-\ndepth assessment of how well AI models can process and comprehend complex \nfinancial information.\n \nThe ongoing research has highlighted several challenges, such as the high computational cost of fine\n-\ntuning \nmodels and the difficulties \nin handling complex document structures. However, these challenges also present \nopportunities for future improvements.\n \nRAG systems have demonstrated significant potential in improving the performance of large language models, \nespecially in knowledge\n-\nintens\nive fields such as finance. By incorporating relevant retrieved documents, RAG \nsystems enable LLMs to produce more precise and contextually relevant answers.\n \nThe future direction of \nimproving retriever quality through fine\n-\ntuning or adopting hybrid RAG ret\nrieval methods could lead to even \ngreater advancements, enabling more precise and reliable retrieval of financial data.\n \nAs research advances, integrating sophisticated RAG architectures and enhancing retrieval strategies will be \nessential for addressing \nexisting limitations and advancing the capabilities of AI systems in financial analysis.\n \nThese advancements will ensure that AI models can provide more accurate, reliable, and insightful financial \ninformation, ultimately aiding analysts, investors, and oth\ner stakeholders in making informed decisions.\n \n \n8.\n \nREFERENCES\n \n[1]\n \nY. Gao \net al.\n, \u201cRetrieval\n-\nAugmented Generation for Large Language Models: A Survey,\u201d Mar. 27, 2024, \narXiv\n: arXiv:2312.1\n0997. Accessed: Jul. 01, 2024. [Online]. Available: http://arxiv.org/abs/2312.10997\n \n[2]\n \nY. Gao, Y. Xiong, M. Wang, and H. Wang, \u201cModular RAG: Transforming RAG Systems into LEGO\n-\nlike \nReconfigurable Frameworks,\u201d Jul. 25, 2024, \narXiv\n: arXiv:2407.21059. Access\ned: Aug. 21, 2024. [Online]. \nAvailable: http://arxiv.org/abs/2407.21059\n \nKRONIKA JOURNAL(ISSN NO-0023:4923)  VOLUME 25 ISSUE 1 2025\nPAGE NO: 61", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4021, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7d8c1a28-2e61-4449-925d-25f15c28925b": {"__data__": {"id_": "7d8c1a28-2e61-4449-925d-25f15c28925b", "embedding": null, "metadata": {"page_label": "10", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9953c556-80a8-4412-8976-69f80e5b5eb1", "node_type": "4", "metadata": {"page_label": "10", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "hash": "65faf269cf681eb1ffbf12f0dd6183711fea55d86e2d762172ad0ed2721b2843", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b09f04a3-8968-457e-884a-862d68173a02", "node_type": "1", "metadata": {}, "hash": "e8ca53ddfdde80d67e56b71174015539dabb20bd58a388027422a04a88a63138", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[3]\n \nS. Jeong, J. Baek, S. Cho, S. J. Hwang, and J. C. Park, \u201cAdaptive\n-\nRAG: Learning to Adapt Retrieval\n-\nAugmented Large Language Models through Question Complexity,\u201d Mar. 28, 2024, \narXi\nv\n: \narXiv:2403.14403. Accessed: Jul. 04, 2024. [Online]. Available: http://arxiv.org/abs/2403.14403\n \n[4]\n \nS.\n-\nQ. Yan, J.\n-\nC. Gu, Y. Zhu, and Z.\n-\nH. Ling, \u201cCorrective Retrieval Augmented Generation,\u201d Feb. 16, 2024, \narXiv\n: arXiv:2401.15884. Accessed: Jul. 03, 2024\n. [Online]. Available: http://arxiv.org/abs/2401.15884\n \n[5]\n \nA. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi, \u201cSelf\n-\nRAG: Learning to Retrieve, Generate, and Critique \nthrough Self\n-\nReflection,\u201d Oct. 17, 2023, \narXiv\n: arXiv:2310.11511. Accessed: Jul. 03, 2024\n. [Online]. \nAvailable: http://arxiv.org/abs/2310.11511\n \n[6]\n \nY. Yuan, C. Liu, J. Yuan, G. Sun, S. Li, and M. Zhang, \u201cA Hybrid RAG System with Comprehensive \nEnhancement on Complex Reasoning,\u201d Aug. 09, 2024, \narXiv\n: arXiv:2408.05141. Accessed: Aug. 21, 2024. \n[O\nnline]. Available: http://arxiv.org/abs/2408.05141\n \n[7]\n \nT. Guo \net al.\n, \u201cLarge Language Model based Multi\n-\nAgents: A Survey of Progress and Challenges,\u201d Apr. 18, \n2024, \narXiv\n: arXiv:2402.01680. doi: 10.48550/arXiv.2402.01680.\n \n[8]\n \nG. M. Rosa, R. C. Rodrigues, R\n. Lotufo, and R. Nogueira, \u201cYes, BM25 is a Strong Baseline for Legal Case \nRetrieval,\u201d Oct. 25, 2021, \narXiv\n: arXiv:2105.05686. Accessed: Aug. 25, 2024. [Online]. Available: \nhttp://arxiv.org/abs/2105.05686\n \n[9]\n \nG. Salton and C. Buckley, \u201cTerm\n-\nweighting approa\nches in automatic text retrieval,\u201d \nInformation Processing \n& Management\n, vol. 24, no. 5, pp. 513\n\u2013\n523, Jan. 1988, doi: 10.1016/0306\n-\n4573(88)90021\n-\n0.\n \n[10]\n \nP. Zhao \net al.\n, \u201cRetrieval\n-\nAugmented Generation for AI\n-\nGenerated Content: A Survey,\u201d Jun. 21, 2024, \narXi\nv\n: \narXiv:2402.19473. Accessed: Aug. 21, 2024. [Online]. Available: http://arxiv.org/abs/2402.19473\n \n[11]\n \nS. Schulhoff \net al.\n, \u201cThe Prompt Report: A Systematic Survey of Prompting Techniques,\u201d Jul. 14, 2024, \narXiv\n: arXiv:2406.06608. doi: 10.48550/arXiv.2406.\n06608.\n \n[12]\n \nP. Sahoo, A. K. Singh, S. Saha, V. Jain, S. Mondal, and A. Chadha, \u201cA Systematic Survey of Prompt \nEngineering in Large Language Models: Techniques and Applications,\u201d Feb. 05, 2024, \narXiv\n: \narXiv:2402.07927. doi: 10.48550/arXiv.2402.07927.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2337, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b09f04a3-8968-457e-884a-862d68173a02": {"__data__": {"id_": "b09f04a3-8968-457e-884a-862d68173a02", "embedding": null, "metadata": {"page_label": "10", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9953c556-80a8-4412-8976-69f80e5b5eb1", "node_type": "4", "metadata": {"page_label": "10", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "hash": "65faf269cf681eb1ffbf12f0dd6183711fea55d86e2d762172ad0ed2721b2843", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d8c1a28-2e61-4449-925d-25f15c28925b", "node_type": "1", "metadata": {"page_label": "10", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "hash": "38731009cd39a2e1b1c433e22d8f74a23121c52ec3378797cc756f358e7f2cb8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da64c7b1-2774-43a8-8a8c-38f94c9fcb28", "node_type": "1", "metadata": {}, "hash": "dba51baa39a01e53d648427d6489acea3b00acd87cabeac79e1cde6468863c3d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Accessed: Aug. 21, 2024. [Online]. Available: http://arxiv.org/abs/2402.19473\n \n[11]\n \nS. Schulhoff \net al.\n, \u201cThe Prompt Report: A Systematic Survey of Prompting Techniques,\u201d Jul. 14, 2024, \narXiv\n: arXiv:2406.06608. doi: 10.48550/arXiv.2406.\n06608.\n \n[12]\n \nP. Sahoo, A. K. Singh, S. Saha, V. Jain, S. Mondal, and A. Chadha, \u201cA Systematic Survey of Prompt \nEngineering in Large Language Models: Techniques and Applications,\u201d Feb. 05, 2024, \narXiv\n: \narXiv:2402.07927. doi: 10.48550/arXiv.2402.07927.\n \n[13]\n \nV. Venerito, D. Lalwani, S. Del Vescovo, F. Iannone, and L. Gupta, \u201cPrompt engineering: The next big skill \nin rheumatology research,\u201d \nInternational Journal of Rheumatic Diseases\n, vol. 27, no. 5, p. e15157, 2024, \ndoi: 10.1111/1756\n-\n185X.15157.\n \n[14]\n \nY. Li, \u201cA\n \nPractical Survey on Zero\n-\nshot Prompt Design for In\n-\ncontext Learning,\u201d in \nProceedings of the \nConference Recent Advances in Natural Language Processing \n-\n \nLarge Language Models for Natural \nLanguage Processings\n, 2023, pp. 641\n\u2013\n647. doi: 10.26615/978\n-\n954\n-\n452\n-\n09\n2\n-\n2_069.\n \n[15]\n \nC. Toukmaji, \u201cFew\n-\nShot Cross\n-\nLingual Transfer for Prompting Large Language Models in Low\n-\nResource \nLanguages,\u201d Mar. 09, 2024, \narXiv\n: arXiv:2403.06018. doi: 10.48550/arXiv.2403.06018.\n \n[16]\n \nW. X. Zhao \net al.\n, \u201cA Survey of Large Language Models,\u201d\n \nNov. 24, 2023, \narXiv\n: arXiv:2303.18223. doi: \n10.48550/arXiv.2303.18223.\n \n[17]\n \nZ. Yuan \net al.\n, \u201cLLM Inference Unveiled: Survey and Roofline Model Insights,\u201d arXiv.org. Accessed: Aug. \n26, 2024. [Online]. Available: https://arxiv.org/abs/2402.16363v6\n \n[18]\n \nP. \nIslam, A. Kannappan, D. Kiela, R. Qian, N. Scherrer, and B. Vidgen, \u201cFinanceBench: A New Benchmark \nfor Financial Question Answering,\u201d Nov. 20, 2023, \narXiv\n: arXiv:2311.11944. Accessed: Jul. 12, 2024. \n[Online]. Available: http://arxiv.org/abs/2311.11944\n \n[19]\n \nA. J. Yepes, Y. You, J. Milczek, S. Laverde, and R. Li, \u201cFinancial Report Chunking for Effective Retrieval \nAugmented Generation,\u201d Mar. 16, 2024, \narXiv\n: arXiv:2402.05131. Accessed: Aug. 04, 2024. [Online]. \nAvailable: http://arxiv.org/abs/2402.05131\n \n[20]\n \nS.\n \nSetty, K. Jijo, E. Chung, and N. Vidra, \u201cImproving Retrieval for RAG based Question Answering Models \non Financial Documents,\u201d Mar. 22, 2024, \narXiv\n: arXiv:2404.07221. Accessed: Jul. 01, 2024. [Online]. \nAvailable: http://arxiv.org/abs/2404.07221\n \n[21]\n \nZ. Ngu\nyen \net al.", "mimetype": "text/plain", "start_char_idx": 1836, "end_char_idx": 4247, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "da64c7b1-2774-43a8-8a8c-38f94c9fcb28": {"__data__": {"id_": "da64c7b1-2774-43a8-8a8c-38f94c9fcb28", "embedding": null, "metadata": {"page_label": "10", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9953c556-80a8-4412-8976-69f80e5b5eb1", "node_type": "4", "metadata": {"page_label": "10", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "hash": "65faf269cf681eb1ffbf12f0dd6183711fea55d86e2d762172ad0ed2721b2843", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b09f04a3-8968-457e-884a-862d68173a02", "node_type": "1", "metadata": {"page_label": "10", "file_name": "5-KKJ2327.pdf", "file_path": "C:\\Users\\kazuki\\graphLM\\backend\\uploaded_files\\a9ce9297-a6a0-4aa1-b83e-6216c230c62d\\5-KKJ2327.pdf", "file_type": "application/pdf", "file_size": 290635, "creation_date": "2025-06-30", "last_modified_date": "2025-06-30"}, "hash": "c5892579aba72df9aa035510bd12c57b0065af57884ef3790238ce68114ac2a0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "16, 2024, \narXiv\n: arXiv:2402.05131. Accessed: Aug. 04, 2024. [Online]. \nAvailable: http://arxiv.org/abs/2402.05131\n \n[20]\n \nS.\n \nSetty, K. Jijo, E. Chung, and N. Vidra, \u201cImproving Retrieval for RAG based Question Answering Models \non Financial Documents,\u201d Mar. 22, 2024, \narXiv\n: arXiv:2404.07221. Accessed: Jul. 01, 2024. [Online]. \nAvailable: http://arxiv.org/abs/2404.07221\n \n[21]\n \nZ. Ngu\nyen \net al.\n, \u201cEnhancing Q&A with Domain\n-\nSpecific Fine\n-\nTuning and Iterative Reasoning: A \nComparative Study,\u201d Apr. 19, 2024, \narXiv\n: arXiv:2404.11792. Accessed: Aug. 20, 2024. [Online]. Available: \nhttp://arxiv.org/abs/2404.11792\n \n[22]\n \nB. Peng \net al.\n, \u201cGraph Ret\nrieval\n-\nAugmented Generation: A Survey,\u201d Aug. 15, 2024, \narXiv\n: \narXiv:2408.08921. doi: 10.48550/arXiv.2408.08921.\n \n[23]\n \nT. Procko and O. Ochoa, \u201cGraph Retrieval\n-\nAugmented Generation for Large Language Models: A Survey,\u201d \nJul. 13, 2024, \nRochester, NY\n: 4895062. \nAccessed: Aug. 21, 2024. [Online]. Available: \nhttps://papers.ssrn.com/abstract=4895062\n \n[24]\n \nV. Karpukhin \net al.\n, \u201cDense Passage Retrieval for Open\n-\nDomain Question Answering,\u201d Sep. 30, 2020, \narXiv\n: \narXiv:2004.04906. doi: 10.48550/arXiv.2004.04906.\n \n \n \nKRONIKA JOURNAL(ISSN NO-0023:4923)  VOLUME 25 ISSUE 1 2025\nPAGE NO: 62", "mimetype": "text/plain", "start_char_idx": 3842, "end_char_idx": 5115, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}}